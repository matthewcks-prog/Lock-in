# =============================================================================
# Lock-in Backend - Docker Compose for Local Development
# Use this to test the containerized backend before Azure deployment
# =============================================================================

services:
  backend:
    build:
      context: .
      dockerfile: backend/Dockerfile
    container_name: lock-in-backend
    ports:
      - '3000:3000'
    environment:
      # Environment selection (ALWAYS development for local Docker testing)
      # This overrides any NODE_ENV set in the Dockerfile
      - NODE_ENV=development
      - PORT=3000

      # Development Supabase ONLY (never mix dev/prod in local env)
      # Backend will use *_DEV variables when NODE_ENV=development
      - SUPABASE_URL_DEV=${SUPABASE_URL_DEV}
      - SUPABASE_SERVICE_ROLE_KEY_DEV=${SUPABASE_SERVICE_ROLE_KEY_DEV}
      - SUPABASE_ANON_KEY_DEV=${SUPABASE_ANON_KEY_DEV:-}

      # Azure OpenAI (primary)
      - AZURE_OPENAI_API_KEY=${AZURE_OPENAI_API_KEY:-}
      - AZURE_OPENAI_ENDPOINT=${AZURE_OPENAI_ENDPOINT:-}
      - AZURE_OPENAI_API_VERSION=${AZURE_OPENAI_API_VERSION:-}
      - AZURE_OPENAI_CHAT_DEPLOYMENT=${AZURE_OPENAI_CHAT_DEPLOYMENT:-}
      - AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT=${AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT:-}
      - AZURE_OPENAI_TRANSCRIPTION_DEPLOYMENT=${AZURE_OPENAI_TRANSCRIPTION_DEPLOYMENT:-}

      # OpenAI (fallback)
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - OPENAI_FALLBACK_ENABLED=${OPENAI_FALLBACK_ENABLED:-true}
      - OPENAI_MODEL=${OPENAI_MODEL:-}
      - OPENAI_EMBEDDINGS_MODEL=${OPENAI_EMBEDDINGS_MODEL:-}
      - OPENAI_TRANSCRIPTION_MODEL=${OPENAI_TRANSCRIPTION_MODEL:-}

      # Sentry (optional)
      - SENTRY_DSN=${SENTRY_DSN:-}

      # Transcription
      - TRANSCRIPTION_TEMP_DIR=/tmp/transcripts

      # Rate limiting (optional overrides)
      - DAILY_REQUEST_LIMIT=${DAILY_REQUEST_LIMIT:-100}
      - TRANSCRIPT_DAILY_JOB_LIMIT=${TRANSCRIPT_DAILY_JOB_LIMIT:-20}
      - TRANSCRIPT_MAX_CONCURRENT_JOBS=${TRANSCRIPT_MAX_CONCURRENT_JOBS:-3}
    volumes:
      # Mount temp volume for transcription jobs (optional, container has ephemeral storage)
      - transcripts-temp:/tmp/transcripts
    healthcheck:
      test: ['CMD', 'curl', '-f', 'http://localhost:3000/health']
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    restart: unless-stopped
    # Resource limits (similar to Azure Container Apps)
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 1G

volumes:
  transcripts-temp:
    driver: local

# =============================================================================
# Usage:
#
# 1. Create .env file in project root (NOT backend/.env) with DEVELOPMENT credentials:
#
#    SUPABASE_URL_DEV=https://your-dev-project.supabase.co
#    SUPABASE_SERVICE_ROLE_KEY_DEV=eyJ...
#    SUPABASE_ANON_KEY_DEV=eyJ...
#    AZURE_OPENAI_API_KEY=your-key
#    AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
#    AZURE_OPENAI_CHAT_DEPLOYMENT=gpt-4o-mini
#    OPENAI_API_KEY=sk-... (optional fallback)
#    SENTRY_DSN=https://...@sentry.io/...  (optional)
#
# 2. Start Docker Desktop
#
# 3. Start container:
#    docker-compose up --build
#
# 4. Test endpoint:
#    curl http://localhost:3000/health
#
# ⚠️ SECURITY: Docker Compose is for LOCAL DEVELOPMENT ONLY.
#    - Never use production credentials
#    - For production deployment, use Azure Container Apps with Key Vault
#    - docker-compose.yml should ONLY reference *_DEV environment variables
# =============================================================================
