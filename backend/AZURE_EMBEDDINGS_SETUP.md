# Azure Embeddings Setup Documentation

## âœ… Configuration Summary

Your Azure OpenAI embeddings are now properly configured and working!

### Resource Details
- **Resource Name**: `lockin-study-assistant-openai`
- **Resource Group**: `lock-in-dev`
- **Location**: `australiaeast`
- **Endpoint**: `https://lock-in-openai-dev.openai.azure.com/`
- **Subscription**: Azure for Students ($100 credit)

### Deployment Details
- **Model**: `text-embedding-3-small` (version 1)
- **Deployment Name**: `text-embedding-3-small`
- **Capacity**: 120K TPM (Tokens Per Minute)
- **SKU**: Standard
- **API Version**: `2024-02-01`

### Rate Limits & Quotas
- **Requests**: 120 requests per 10 seconds
- **Tokens**: 120,000 tokens per minute
- **Batch Size**: Up to 2,048 embeddings per request
- **Vector Dimensions**: 1536 (default) or 512 (reduced)

### Cost Structure
- **Pricing**: $0.02 per 1M tokens
- **Average Token Count**: ~1 token per 4 characters
- **Estimated Cost per 1K notes** (assuming 500 chars each): ~$0.0025

---

## ðŸ—ï¸ Industry Best Practices Implemented

### 1. **Connection Management**
- âœ… 60-second timeout for embeddings operations
- âœ… Automatic retry with exponential backoff (3 attempts)
- âœ… Connection pooling via Azure SDK
- âœ… Graceful error handling with detailed diagnostics

### 2. **Error Handling**
- âœ… Specific error types (auth, rate limit, network, service)
- âœ… Actionable error messages with troubleshooting steps
- âœ… Fallback to OpenAI when Azure unavailable
- âœ… Error tracking and logging

### 3. **Performance & Scalability**
- âœ… Batch processing support (up to 2,048 items)
- âœ… Optimal capacity allocation (120K TPM)
- âœ… Usage tracking and cost monitoring
- âœ… Efficient token utilization

### 4. **Monitoring & Observability**
- âœ… Usage statistics tracking (requests, tokens, cost)
- âœ… Health check endpoints (planned)
- âœ… Diagnostic tools for troubleshooting
- âœ… Structured logging with context

### 5. **Security**
- âœ… API keys stored in environment variables
- âœ… Sensitive data redacted in logs
- âœ… Secure credential rotation support
- âœ… HTTPS-only connections

---

## ðŸ“Š Usage Monitoring

### Check Your Usage
Run the test script to see current statistics:
```bash
cd backend
node test-embeddings.js
```

### View Azure Portal Metrics
1. Go to [Azure Portal](https://portal.azure.com)
2. Navigate to: **lock-in-dev** > **lockin-study-assistant-openai**
3. Click **Metrics** in the left menu
4. View: Total Calls, Token Usage, Latency

### Monitor Costs
1. Azure Portal > **Cost Management + Billing**
2. Filter by Resource Group: `lock-in-dev`
3. Review daily/monthly spending

---

## ðŸ”§ Maintenance & Troubleshooting

### Rotate API Keys
```bash
# Regenerate key1
az cognitiveservices account keys regenerate \
  --name lockin-study-assistant-openai \
  --resource-group lock-in-dev \
  --key-name key1

# Get new key
az cognitiveservices account keys list \
  --name lockin-study-assistant-openai \
  --resource-group lock-in-dev

# Update backend/.env with new AZURE_OPENAI_API_KEY
```

### Check Deployment Status
```bash
az cognitiveservices account deployment list \
  --name lockin-study-assistant-openai \
  --resource-group lock-in-dev
```

### View Quota Usage
```bash
az cognitiveservices usage list \
  --name lockin-study-assistant-openai \
  --resource-group lock-in-dev
```

### Run Diagnostics
```bash
cd backend
node test-azure-direct.js
```

---

## ðŸš€ Scaling Recommendations

### Current Capacity
- **120K TPM**: Good for development and moderate production load
- **Estimated capacity**: ~2,000 embedding requests per minute

### When to Scale Up
If you hit rate limits or need higher throughput:

```bash
# Increase to 240K TPM (double capacity)
az cognitiveservices account deployment create \
  --name lockin-study-assistant-openai \
  --resource-group lock-in-dev \
  --deployment-name text-embedding-3-small \
  --model-name text-embedding-3-small \
  --model-version "1" \
  --model-format OpenAI \
  --sku-capacity 240 \
  --sku-name "Standard"
```

### Production Recommendations
1. **Monitor usage patterns** - Track peak hours and adjust capacity
2. **Enable auto-scaling** - Use Azure Monitor alerts
3. **Implement caching** - Cache embeddings for repeated content
4. **Batch operations** - Group multiple embeddings together
5. **Use reduced dimensions** - Consider 512 dimensions for lower latency

---

## ðŸ” Environment Configuration

### backend/.env (Development)
```env
AZURE_OPENAI_API_KEY=your-azure-openai-api-key-here
AZURE_OPENAI_ENDPOINT=https://lock-in-openai-dev.openai.azure.com/
AZURE_OPENAI_API_VERSION=2024-02-01
AZURE_OPENAI_EMBEDDINGS_DEPLOYMENT=text-embedding-3-small

# OpenAI Fallback (Optional)
OPENAI_API_KEY=sk-proj-...
OPENAI_EMBEDDINGS_MODEL=text-embedding-3-small
OPENAI_FALLBACK_ENABLED=true
```

### Production Setup
For production deployment:
1. Create separate Azure resource for prod
2. Use separate API keys (never share dev/prod)
3. Enable private endpoints for enhanced security
4. Set up Azure Key Vault for secret management
5. Configure network restrictions if needed

---

## ðŸ“ˆ Performance Optimization

### Batch Processing Example
```javascript
// Instead of multiple calls
for (const note of notes) {
  await embedText(note.content); // âŒ Slow
}

// Use batch processing
const texts = notes.map(n => n.content);
const embeddings = await embedTexts(texts); // âœ… Fast
```

### Caching Strategy
```javascript
// Implement caching to avoid redundant embeddings
const embeddingCache = new Map();

function getOrCreateEmbedding(text) {
  if (embeddingCache.has(text)) {
    return embeddingCache.get(text);
  }
  const embedding = await embedText(text);
  embeddingCache.set(text, embedding);
  return embedding;
}
```

### Reduced Dimensions for Speed
```javascript
// Use 512 dimensions for faster processing (slight accuracy trade-off)
const embedding = await client.embed(text, { dimensions: 512 });
```

---

## ðŸŽ¯ Next Steps

1. **Enable health monitoring** - Uncomment healthRoutes in app.js
2. **Set up alerts** - Configure Azure Monitor for quota warnings
3. **Implement caching** - Cache embeddings in Redis/memory
4. **Monitor costs** - Set budget alerts in Azure
5. **Production deployment** - Create separate prod resource

---

## ðŸ“ž Support Resources

- **Azure OpenAI Documentation**: https://learn.microsoft.com/azure/ai-services/openai/
- **Pricing Calculator**: https://azure.microsoft.com/pricing/calculator/
- **API Reference**: https://learn.microsoft.com/azure/ai-services/openai/reference
- **Support**: Azure Portal > Support + troubleshooting

---

## âœ… Verification Checklist

- [x] Azure OpenAI resource created
- [x] Custom subdomain configured
- [x] text-embedding-3-small deployed
- [x] 120K TPM capacity allocated
- [x] API keys updated in .env
- [x] Connection tested and working
- [x] Error handling implemented
- [x] Usage tracking enabled
- [x] Fallback to OpenAI configured
- [x] Documentation completed

**Status**: âœ… Ready for production use!
